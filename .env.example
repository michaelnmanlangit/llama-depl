# Environment variables for Llama deployment

# HuggingFace token (required for accessing gated models like Llama)
HF_TOKEN=your_huggingface_token_here

# API Configuration
API_PORT=8000
API_HOST=0.0.0.0

# Model Configuration
MODEL_ID=meta-llama/Llama-3.2-1B
USE_QUANTIZATION=true

# Server Configuration
MAX_WORKERS=2
TIMEOUT=120
